{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Engineer Nanodegree\n",
    "## Capstone Project\n",
    "## Mushroom Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There exist many edible, high quality mushrooms which are rich with vitamins and minerals and have great value at market. However, some of the mushrooms are toxic, which can cause different type of health problems if consumed, and a small number of them is even deadly. Throughout this project, we want to classify the mushrooms and find out which ones are edible, and which ones are toxic.\n",
    "\n",
    "We will analyse a dataset containing mushroom information, and train a few different supervised learning algorithms in order to classify the new inputs as poisonous or edible. We will run unsupervised learning techniques in order to see what kind of trait correlation exists between the mushroom, for the sake of improving our knowledge for feature selection and transformation. In the end, we will test and tune our supervised learning algorithms, in order to see which one is giving the best performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the Data\n",
    "Run the code cell below to load necessary Python libraries and load the student data. Note that the last column from this dataset, `'passed'`, will be our target label (whether the student graduated or didn't graduate). All other columns are features about each student."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mushroom data read successfully!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "mushroom_data = pd.read_csv(\"mushroom_dataset.csv\")\n",
    "print \"Mushroom data read successfully!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration\n",
    "Let's begin by investigating our dataset. How many instances there are in the dataset, how many of them are poisonous, and how many of them are edible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of mushrooms: 8124\n",
      "Number of features: 22\n",
      "Number of edible mushrooms: 4208\n",
      "Number of poisonous mushrooms: 3916\n",
      "51.80% of mushrooms are edible\n",
      "48.20% of mushrooms are poisonous\n"
     ]
    }
   ],
   "source": [
    "number_of_mushrooms = len(mushroom_data.count(axis=1))\n",
    "\n",
    "number_of_features = len(mushroom_data.count(axis=0))-1\n",
    "\n",
    "number_of_edible = len(mushroom_data[mushroom_data.type == \"e\"])\n",
    "\n",
    "number_of_poisonous = len(mushroom_data[mushroom_data.type == \"p\"])\n",
    "\n",
    "edible_percentile = (number_of_edible/float(number_of_mushrooms))*100\n",
    "\n",
    "poisonous_percentile = 100-edible_percentile\n",
    "\n",
    "print \"Total number of mushrooms: {}\".format(number_of_mushrooms)\n",
    "print \"Number of features: {}\".format(number_of_features)\n",
    "print \"Number of edible mushrooms: {}\".format(number_of_edible)\n",
    "print \"Number of poisonous mushrooms: {}\".format(number_of_poisonous)\n",
    "print \"{:.2f}% of mushrooms are edible\".format(edible_percentile)\n",
    "print \"{:.2f}% of mushrooms are poisonous\".format(poisonous_percentile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Data\n",
    "In this section, we will prepare the data for modeling, training and testing.\n",
    "\n",
    "### Missing values\n",
    "We know that missing values are noted as '?' in the dataset. Let's examine how many missing values we have and for which features is that the case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2480 missing values from the following features: Set(['stalk-root'])\n"
     ]
    }
   ],
   "source": [
    "def detect_missing_values(data):\n",
    "    from sets import Set\n",
    "    \n",
    "    df = pd.DataFrame(index = data.index)\n",
    "    features_with_missing_values = Set()\n",
    "    number_of_missing_values = 0\n",
    "    \n",
    "    for col, col_data in data.iteritems():\n",
    "        \n",
    "        missing_values_in_column = len(data[col][col_data=='?'])\n",
    "        \n",
    "        if missing_values_in_column > 0:\n",
    "            features_with_missing_values.add(col)\n",
    "            number_of_missing_values+=missing_values_in_column\n",
    "        \n",
    "        \n",
    "    print \"There are {} missing values from the following features: {}\".format(number_of_missing_values, features_with_missing_values)\n",
    "        \n",
    "    \n",
    "    \n",
    "detect_missing_values(mushroom_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our missing values are contained only in stalk-root feature. Let's keep them for now, and if they make us problems later, we will remove those records with missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label encoding\n",
    "In order to perform predictions, we will need to turn our data into numeric, and for that we will be using label encoder which is a part of sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "mushroom_data = mushroom_data.apply(preprocessing.LabelEncoder().fit_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify feature and target columns\n",
    "Now, let's split our target column from feature columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature columns:\n",
      "Index([u'cap-shape', u'cap-surface', u'cap-color', u'bruises', u'odor',\n",
      "       u'gill-attachment', u'gill-spacing', u'gill-size', u'gill-color',\n",
      "       u'stalk-shape', u'stalk-root', u'stalk-surface-above-ring',\n",
      "       u'stalk-surface-below-ring', u'stalk-color-above-ring',\n",
      "       u'stalk-color-below-ring', u'veil-type', u'veil-color', u'ring-number',\n",
      "       u'ring-type', u'spore-print-color', u'population', u'habitat'],\n",
      "      dtype='object')\n",
      "\n",
      "Target column: type\n",
      "\n",
      "Feature values:\n",
      "   cap-shape  cap-surface  cap-color  bruises  odor  gill-attachment  \\\n",
      "0          5            2          4        1     6                1   \n",
      "1          5            2          9        1     0                1   \n",
      "2          0            2          8        1     3                1   \n",
      "3          5            3          8        1     6                1   \n",
      "4          5            2          3        0     5                1   \n",
      "\n",
      "   gill-spacing  gill-size  gill-color  stalk-shape   ...     \\\n",
      "0             0          1           4            0   ...      \n",
      "1             0          0           4            0   ...      \n",
      "2             0          0           5            0   ...      \n",
      "3             0          1           5            0   ...      \n",
      "4             1          0           4            1   ...      \n",
      "\n",
      "   stalk-surface-below-ring  stalk-color-above-ring  stalk-color-below-ring  \\\n",
      "0                         2                       7                       7   \n",
      "1                         2                       7                       7   \n",
      "2                         2                       7                       7   \n",
      "3                         2                       7                       7   \n",
      "4                         2                       7                       7   \n",
      "\n",
      "   veil-type  veil-color  ring-number  ring-type  spore-print-color  \\\n",
      "0          0           2            1          4                  2   \n",
      "1          0           2            1          4                  3   \n",
      "2          0           2            1          4                  3   \n",
      "3          0           2            1          4                  2   \n",
      "4          0           2            1          0                  3   \n",
      "\n",
      "   population  habitat  \n",
      "0           3        5  \n",
      "1           2        1  \n",
      "2           2        3  \n",
      "3           3        5  \n",
      "4           0        1  \n",
      "\n",
      "[5 rows x 22 columns]\n"
     ]
    }
   ],
   "source": [
    "target_column = mushroom_data.columns[0]\n",
    "feature_columns = mushroom_data.columns[1:]\n",
    "\n",
    "print \"Feature columns:\\n{}\".format(feature_columns)\n",
    "print \"\\nTarget column: {}\".format(target_column)\n",
    "\n",
    "X_all = mushroom_data[feature_columns]\n",
    "y_all = mushroom_data[target_column]\n",
    "\n",
    "print \"\\nFeature values:\"\n",
    "print X_all.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection\n",
    "Since we have 22 features for every record in the dataset, let's analyse which features are irrelevant and can be removed from it. We will run a simple decision tree classifier on every feature and report how good the prediction is. Those features that can already be predicted, are not necessary for our target prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cap-shape : 0.101920236337\n",
      "cap-surface : 0.232890201871\n",
      "cap-color : 0.110782865583\n",
      "bruises : 1.0\n",
      "odor : 0.70113244707\n",
      "gill-attachment : 0.997045790251\n",
      "gill-spacing : 0.981290004924\n",
      "gill-size : 1.0\n",
      "gill-color : 0.238306253077\n",
      "stalk-shape : 1.0\n",
      "stalk-root : 1.0\n",
      "stalk-surface-above-ring : 0.663220088626\n",
      "stalk-surface-below-ring : 0.65780403742\n",
      "stalk-color-above-ring : 0.439684884293\n",
      "stalk-color-below-ring : 0.439684884293\n",
      "veil-type : 1.0\n",
      "veil-color : 0.977843426883\n",
      "ring-number : 1.0\n",
      "ring-type : 1.0\n",
      "spore-print-color : 0.563269325455\n",
      "population : 0.374692269818\n",
      "habitat : 0.472673559823\n"
     ]
    }
   ],
   "source": [
    "def test_feature_accuracy(X):\n",
    "    \n",
    "    for col, col_data in X.iteritems():\n",
    "        target_feature = X[col]\n",
    "        new_x = X.drop(col, axis=1)\n",
    "    \n",
    "        from sklearn.cross_validation import train_test_split\n",
    "        X_train, X_test, y_train, y_test = train_test_split(new_x, target_feature, test_size=0.25, random_state=42)\n",
    "\n",
    "        from sklearn.tree import DecisionTreeClassifier\n",
    "        classifier = DecisionTreeClassifier(random_state=42)\n",
    "        classifier.fit(X_train, y_train)\n",
    "\n",
    "        score = classifier.score(X_test, y_test)\n",
    "        \n",
    "        if score>0.9:\n",
    "            X = X.drop(col, axis=1)\n",
    "\n",
    "        print \"{} : {}\".format(col, score)\n",
    "        \n",
    "    return  X\n",
    "    \n",
    "    \n",
    "X_all = test_feature_accuracy(X_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After removing those cells that can be predicted with score greater than 0.9, we now have 12 features and the number of features is significally reduced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Evaluating Models\n",
    "Now, our data is prepared and we can begin modeling, training and evaluating our models. We will choose 3 supervised learning models, and try fitting our data to them. Then, we will see how do they perform and tune them accordingly. For those classifiers that have class_weight and sample_weight parameters, we will tune them in order to implement cost sensitive learning (for example, it is better to predict edible mushroom as poisonous than poisonous mushroom as edible). We will evaluate our model with accuracy score and produce the tables which show the training set size, time, prediction time, accuracy score on training and accuracy score on testing set. For this problem, we will choose the following three algorithms:\n",
    "\n",
    "- SVM\n",
    "- Stochastic Gradient Descent\n",
    "- Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Support Vector Machines\n",
    "The reason I choose SVMs is that the real world application of them is binary classification. Since we are dealing with binary classification in this problem, we will expect this model to perform well. The idea behind SVM is to find the greatest margin between two different sets. It is computing it mathematically to find the hyperplane which will separate the data with the greatest margin. If the data is not linearly separable in 2D, it will hit up higher dimension and then try to separate it. SVM performs poorly with large datasets and with datasets with lots of noise. This is not the case in our problem, so I think that this is good option.\n",
    "\n",
    "- Stochastic Gradient Descent\n",
    "Real-world application: We can use neural networks to recognize handwritten digits. The idea is to estimate the gradient by computing the part of the gradient for a small sample of randomly chosen training inputs. By averaging over this small sample it turns out that we can quickly get a good estimate of the true gradient, and this helps speed up gradient descent, and thus learning. Pros are that it is a fairly well studied algorithm, so for the most part the problems with GD have solutions. However, sometimes calculating the gradient can be very expensive or intractable if the size of our data is large. Stochastic gradient descent is a good option which theoretically will converge to a local max. Since we do not have lots of data in this problem, and with tuning parameters, this should be a good option for us.\n",
    "\n",
    "- Random Forest\n",
    "While reading papers regarding this problem we are tackling, we have seen that the decision trees can give 100% accuracy while predicting the mushrooms as edible or poisonous. Random forest is an ensemble of decision trees. Each decision tree is constructed by using a random subset of the training data. After we have trained our forest, we can pass each test row through it, in order to output a prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "Let's create three helper functions which we will be using for training and testing our models. Create confusion matrix function is pretty straightforward. It creates the confusion matrix based on true and predicted values. Train classifier function will take a classifier as a param and train it with the provided data. Predict labels will take as input fit classifier, features and a target labeling. Then it will make predictions using the accuracy score. Train predict will take as input a classifier, training and testing data and it will perform train classifier and predict labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_confusion_matrix(true, pred):\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    print \"----------------\"\n",
    "    print \"Confusion matrix\\n\"\n",
    "    print confusion_matrix(true, pred)\n",
    "    print \"\\n----------------\\n\"\n",
    "\n",
    "def train_classifier(clf, X_train, y_train):\n",
    "\n",
    "    start = time()\n",
    "    clf.fit(X_train, y_train)\n",
    "    end = time()\n",
    "    print \"Trained model in {:.4f} seconds\".format(end - start)\n",
    "    \n",
    "def predict_labels(clf, features, target, confusion_matrix=False):\n",
    "\n",
    "    start = time()\n",
    "    y_pred = clf.predict(features)\n",
    "    end = time()\n",
    "\n",
    "    print \"Made predictions in {:.4f} seconds.\".format(end - start)\n",
    "    \n",
    "    if confusion_matrix:\n",
    "        create_confusion_matrix(target.values, y_pred)\n",
    "    \n",
    "    return accuracy_score(target.values, y_pred)\n",
    "\n",
    "\n",
    "def train_predict(clf, X_train, y_train, X_test, y_test, confusion_matrix=False):\n",
    "\n",
    "    print \"Training a {} using a training set size of {}. . .\".format(clf.__class__.__name__, len(X_train))\n",
    "    \n",
    "    train_classifier(clf, X_train, y_train)\n",
    "    \n",
    "    print \"Accuracy score for training set: {:.4f}.\".format(predict_labels(clf, X_train, y_train, confusion_matrix))\n",
    "    print \"Accuracy score for test set: {:.4f}.\".format(predict_labels(clf, X_test, y_test, confusion_matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation: Model Performance Metrics\n",
    "Now we will import our supervised learning models and run the train_predict function for each one. We will use different training set sizes (100, 200, 300). We will print confusion matrices for each prediction. Details such as prediction time, accuracy scores, training time etc. will be contained in the tabular results below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SVC: \n",
      "\n",
      "Training a SVC using a training set size of 100. . .\n",
      "Trained model in 0.0012 seconds\n",
      "Made predictions in 0.0005 seconds.\n",
      "Accuracy score for training set: 1.0000.\n",
      "Made predictions in 0.0096 seconds.\n",
      "Accuracy score for test set: 0.8922.\n",
      "Training a SVC using a training set size of 200. . .\n",
      "Trained model in 0.0040 seconds\n",
      "Made predictions in 0.0023 seconds.\n",
      "Accuracy score for training set: 0.9900.\n",
      "Made predictions in 0.0159 seconds.\n",
      "Accuracy score for test set: 0.9222.\n",
      "Training a SVC using a training set size of 300. . .\n",
      "Trained model in 0.0059 seconds\n",
      "Made predictions in 0.0031 seconds.\n",
      "Accuracy score for training set: 0.9933.\n",
      "Made predictions in 0.0143 seconds.\n",
      "Accuracy score for test set: 0.9473.\n",
      "\n",
      "SGDClassifier: \n",
      "\n",
      "Training a SGDClassifier using a training set size of 100. . .\n",
      "Trained model in 0.0005 seconds\n",
      "Made predictions in 0.0001 seconds.\n",
      "Accuracy score for training set: 0.5900.\n",
      "Made predictions in 0.0002 seconds.\n",
      "Accuracy score for test set: 0.5574.\n",
      "Training a SGDClassifier using a training set size of 200. . .\n",
      "Trained model in 0.0005 seconds\n",
      "Made predictions in 0.0001 seconds.\n",
      "Accuracy score for training set: 0.6650.\n",
      "Made predictions in 0.0001 seconds.\n",
      "Accuracy score for test set: 0.6539.\n",
      "Training a SGDClassifier using a training set size of 300. . .\n",
      "Trained model in 0.0005 seconds\n",
      "Made predictions in 0.0001 seconds.\n",
      "Accuracy score for training set: 0.8500.\n",
      "Made predictions in 0.0001 seconds.\n",
      "Accuracy score for test set: 0.8518.\n",
      "\n",
      "RandomForestClassifier: \n",
      "\n",
      "Training a RandomForestClassifier using a training set size of 100. . .\n",
      "Trained model in 0.0291 seconds\n",
      "Made predictions in 0.0012 seconds.\n",
      "Accuracy score for training set: 1.0000.\n",
      "Made predictions in 0.0025 seconds.\n",
      "Accuracy score for test set: 0.9547.\n",
      "Training a RandomForestClassifier using a training set size of 200. . .\n",
      "Trained model in 0.0305 seconds\n",
      "Made predictions in 0.0012 seconds.\n",
      "Accuracy score for training set: 1.0000.\n",
      "Made predictions in 0.0024 seconds.\n",
      "Accuracy score for test set: 0.9769.\n",
      "Training a RandomForestClassifier using a training set size of 300. . .\n",
      "Trained model in 0.0305 seconds\n",
      "Made predictions in 0.0012 seconds.\n",
      "Accuracy score for training set: 1.0000.\n",
      "Made predictions in 0.0029 seconds.\n",
      "Accuracy score for test set: 0.9734.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf_A = SVC(random_state=42)\n",
    "clf_B = SGDClassifier(random_state=42)\n",
    "clf_C = RandomForestClassifier()\n",
    "\n",
    "for clf in [clf_A, clf_B, clf_C]:\n",
    "    print \"\\n{}: \\n\".format(clf.__class__.__name__)\n",
    "    for n in [100, 200, 300]:\n",
    "        train_predict(clf, X_train[:n], y_train[:n], X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tabular Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Classifer 1 - SVC **  \n",
    "\n",
    "| Training Set Size | Training Time | Prediction Time (test) | Accuracy Score (train) | Accuracy Score (test) |\n",
    "| :---------------: | :---------------------: | :--------------------: | :--------------: | :-------------: |\n",
    "| 100               | 0.0012                  | 0.0005                 |   1.0000         | 0.8922          |\n",
    "| 200               | 0.0032                  | 0.0011                 |   0.9900         | 0.9222          |\n",
    "| 300               | 0.0044                  | 0.0026                 |   0.9933         | 0.9473          |\n",
    "\n",
    "** Classifer 2 - SGD **  \n",
    "\n",
    "| Training Set Size | Training Time | Prediction Time (test) | Accuracy Score (train) | Accuracy Score (test) |\n",
    "| :---------------: | :---------------------: | :--------------------: | :--------------: | :-------------: |\n",
    "| 100               | 0.0006                  | 0.0001                 | 0.5900           | 0.5574          |\n",
    "| 200               | 0.0006                  | 0.0001                 | 0.6650           | 0.6539          |\n",
    "| 300               | 0.0006                  | 0.0002                 | 0.8500           | 0.8518          |\n",
    "\n",
    "** Classifer 3 - Random Forests **  \n",
    "\n",
    "| Training Set Size | Training Time | Prediction Time (test) | Accuracy Score (train) | Accuracy Score (test) |\n",
    "| :---------------: | :---------------------: | :--------------------: | :--------------: | :-------------: |\n",
    "| 100               | 0.0237                  | 0.0009                 | 1.0000           | 0.9581          |\n",
    "| 200               | 0.0228                  | 0.0009                 | 1.0000           | 0.9695          |\n",
    "| 300               | 0.0227                  | 0.0010                 | 1.0000           | 0.9665          |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing the Best Model\n",
    "Based on the experiments performed earlier, we have confirmed our assumptions about each model. Random Forests performs the best, as it is ensemble of decision trees. SVMs are performing well as we are dealing with the binary classification here, and Stochastic Gradient Descent is the fastest, but the accuracy score is very low compared to the other two models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Tuning\n",
    "Now, it is time to apply first iteration of tuning our Random Forests model. We will be using grid search (`GridSearchCV`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Made predictions in 0.0043 seconds.\n",
      "----------------\n",
      "Confusion matrix\n",
      "\n",
      "[[3168    0]\n",
      " [ 153 2772]]\n",
      "\n",
      "----------------\n",
      "\n",
      "Tuned model has a training accuracy score of 0.9749.\n",
      "Made predictions in 0.0019 seconds.\n",
      "----------------\n",
      "Confusion matrix\n",
      "\n",
      "[[1040    0]\n",
      " [  46  945]]\n",
      "\n",
      "----------------\n",
      "\n",
      "Tuned model has a testing accuracy score of 0.9774.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "parameters = {'max_depth': [2,4,6], 'min_samples_leaf': [2,5,10], }\n",
    " \n",
    "clf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "accuracy_scorer = make_scorer(accuracy_score)\n",
    "\n",
    "grid_obj = GridSearchCV(clf, parameters, scoring=accuracy_scorer)\n",
    "\n",
    "grid_obj = grid_obj.fit(X_train, y_train)\n",
    "\n",
    "clf = grid_obj.best_estimator_\n",
    "\n",
    "print \"Tuned model has a training accuracy score of {:.4f}.\".format(predict_labels(clf, X_train, y_train, True))\n",
    "print \"Tuned model has a testing accuracy score of {:.4f}.\".format(predict_labels(clf, X_test, y_test, True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrices\n",
    "\n",
    "- Confusion Matrix for training\n",
    "\n",
    "| True Positive | True Negative | False Positive | False Negative |\n",
    "| :---------------:| :--------------------:| :---------------: | :---------------------: |\n",
    "| 3168               | 2772                  | 153               | 0                  |\n",
    "\n",
    "\n",
    "- Confusion Matrix for testing\n",
    "\n",
    "| True Positive | True Negative | False Positive | False Negative |\n",
    "| :---------------:| :--------------------:| :---------------: | :---------------------: |\n",
    "| 1040               | 945                  | 46               | 0                  |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
