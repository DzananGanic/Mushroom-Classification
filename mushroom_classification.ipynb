{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Engineer Nanodegree\n",
    "## Capstone Project\n",
    "## Mushroom Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There exist many edible, high quality mushrooms which are rich with vitamins and minerals and have great value at market. However, some of the mushrooms are toxic, which can cause different type of health problems if consumed, and a small number of them is even deadly. Throughout this project, we want to classify the mushrooms and find out which ones are edible, and which ones are toxic.\n",
    "\n",
    "We will analyse a dataset containing mushroom information, and train a few different supervised learning algorithms in order to classify the new inputs as poisonous or edible. We will run unsupervised learning techniques in order to see what kind of trait correlation exists between the mushroom, for the sake of improving our knowledge for feature selection and transformation. In the end, we will test and tune our supervised learning algorithms, in order to see which one is giving the best performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the Data\n",
    "Let's run the code cell below to load necessary Python libraries and load the mushroom data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mushroom data read successfully!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "mushroom_data = pd.read_csv(\"mushroom_dataset.csv\")\n",
    "print \"Mushroom data read successfully!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration\n",
    "Let's begin by investigating our dataset. How many instances there are in the dataset, how many of them are poisonous, and how many of them are edible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of mushrooms: 8124\n",
      "Number of features: 22\n",
      "Number of edible mushrooms: 4208\n",
      "Number of poisonous mushrooms: 3916\n",
      "51.80% of mushrooms are edible\n",
      "48.20% of mushrooms are poisonous\n"
     ]
    }
   ],
   "source": [
    "number_of_mushrooms = len(mushroom_data.count(axis=1))\n",
    "\n",
    "number_of_features = len(mushroom_data.count(axis=0))-1\n",
    "\n",
    "number_of_edible = len(mushroom_data[mushroom_data.type == \"e\"])\n",
    "\n",
    "number_of_poisonous = len(mushroom_data[mushroom_data.type == \"p\"])\n",
    "\n",
    "edible_percentile = (number_of_edible/float(number_of_mushrooms))*100\n",
    "\n",
    "poisonous_percentile = 100-edible_percentile\n",
    "\n",
    "print \"Total number of mushrooms: {}\".format(number_of_mushrooms)\n",
    "print \"Number of features: {}\".format(number_of_features)\n",
    "print \"Number of edible mushrooms: {}\".format(number_of_edible)\n",
    "print \"Number of poisonous mushrooms: {}\".format(number_of_poisonous)\n",
    "print \"{:.2f}% of mushrooms are edible\".format(edible_percentile)\n",
    "print \"{:.2f}% of mushrooms are poisonous\".format(poisonous_percentile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1408 e mushrooms living in g\n",
      "There are 240 e mushrooms living in l\n",
      "There are 256 e mushrooms living in m\n",
      "There are 136 e mushrooms living in p\n",
      "There are 96 e mushrooms living in u\n",
      "There are 192 e mushrooms living in w\n",
      "There are 1880 e mushrooms living in d\n",
      "There are 740 p mushrooms living in g\n",
      "There are 592 p mushrooms living in l\n",
      "There are 36 p mushrooms living in m\n",
      "There are 1008 p mushrooms living in p\n",
      "There are 272 p mushrooms living in u\n",
      "There are 0 p mushrooms living in w\n",
      "There are 1268 p mushrooms living in d\n"
     ]
    }
   ],
   "source": [
    "# Additional misc statistics\n",
    "\n",
    "def count_habitat(t, h):\n",
    "    return len(mushroom_data[np.all([mushroom_data.type==t, mushroom_data.habitat==h], axis=0)])\n",
    "\n",
    "# How many different mushrooms live in different habitats\n",
    "for t in [\"e\", \"p\"]:\n",
    "    for h in [\"g\", \"l\", \"m\", \"p\", \"u\", \"w\", \"d\"]:\n",
    "        print \"There are {} {} mushrooms living in {}\".format(count_habitat(t,h), t, h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Data\n",
    "In this section, we will prepare the data for modeling, training and testing.\n",
    "\n",
    "### Missing values\n",
    "We know that missing values are noted as '?' in the dataset. Let's examine how many missing values we have and for which features is that the case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2480 missing values from the following features: Set(['stalk-root'])\n"
     ]
    }
   ],
   "source": [
    "def detect_missing_values(data):\n",
    "    from sets import Set\n",
    "    \n",
    "    df = pd.DataFrame(index = data.index)\n",
    "    features_with_missing_values = Set()\n",
    "    number_of_missing_values = 0\n",
    "    \n",
    "    for col, col_data in data.iteritems():\n",
    "        \n",
    "        missing_values_in_column = len(data[col][col_data=='?'])\n",
    "        \n",
    "        if missing_values_in_column > 0:\n",
    "            features_with_missing_values.add(col)\n",
    "            number_of_missing_values+=missing_values_in_column\n",
    "        \n",
    "        \n",
    "    print \"There are {} missing values from the following features: {}\".format(number_of_missing_values, features_with_missing_values)\n",
    "        \n",
    "    \n",
    "    \n",
    "detect_missing_values(mushroom_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our missing values are contained only in stalk-root feature. We will see that in 'Feature Selection' chapter that 'stalk-root' feature can be predicted based on the other features, thus being unnecessary. We will remove it along with other unnecessary features in 'Feature Selection' part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label encoding\n",
    "In order to perform predictions, we will need to turn our data into numeric, and for that we will be using label encoder which is a part of sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "mushroom_data = mushroom_data.apply(preprocessing.LabelEncoder().fit_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify feature and target columns\n",
    "Now, let's split our target column from feature columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature columns:\n",
      "Index([u'cap-shape', u'cap-surface', u'cap-color', u'bruises', u'odor',\n",
      "       u'gill-attachment', u'gill-spacing', u'gill-size', u'gill-color',\n",
      "       u'stalk-shape', u'stalk-root', u'stalk-surface-above-ring',\n",
      "       u'stalk-surface-below-ring', u'stalk-color-above-ring',\n",
      "       u'stalk-color-below-ring', u'veil-type', u'veil-color', u'ring-number',\n",
      "       u'ring-type', u'spore-print-color', u'population', u'habitat'],\n",
      "      dtype='object')\n",
      "\n",
      "Target column: type\n",
      "\n",
      "Feature values:\n",
      "   cap-shape  cap-surface  cap-color  bruises  odor  gill-attachment  \\\n",
      "0          5            2          4        1     6                1   \n",
      "1          5            2          9        1     0                1   \n",
      "2          0            2          8        1     3                1   \n",
      "3          5            3          8        1     6                1   \n",
      "4          5            2          3        0     5                1   \n",
      "\n",
      "   gill-spacing  gill-size  gill-color  stalk-shape   ...     \\\n",
      "0             0          1           4            0   ...      \n",
      "1             0          0           4            0   ...      \n",
      "2             0          0           5            0   ...      \n",
      "3             0          1           5            0   ...      \n",
      "4             1          0           4            1   ...      \n",
      "\n",
      "   stalk-surface-below-ring  stalk-color-above-ring  stalk-color-below-ring  \\\n",
      "0                         2                       7                       7   \n",
      "1                         2                       7                       7   \n",
      "2                         2                       7                       7   \n",
      "3                         2                       7                       7   \n",
      "4                         2                       7                       7   \n",
      "\n",
      "   veil-type  veil-color  ring-number  ring-type  spore-print-color  \\\n",
      "0          0           2            1          4                  2   \n",
      "1          0           2            1          4                  3   \n",
      "2          0           2            1          4                  3   \n",
      "3          0           2            1          4                  2   \n",
      "4          0           2            1          0                  3   \n",
      "\n",
      "   population  habitat  \n",
      "0           3        5  \n",
      "1           2        1  \n",
      "2           2        3  \n",
      "3           3        5  \n",
      "4           0        1  \n",
      "\n",
      "[5 rows x 22 columns]\n"
     ]
    }
   ],
   "source": [
    "target_column = mushroom_data.columns[0]\n",
    "feature_columns = mushroom_data.columns[1:]\n",
    "\n",
    "print \"Feature columns:\\n{}\".format(feature_columns)\n",
    "print \"\\nTarget column: {}\".format(target_column)\n",
    "\n",
    "X_all = mushroom_data[feature_columns]\n",
    "y_all = mushroom_data[target_column]\n",
    "\n",
    "print \"\\nFeature values:\"\n",
    "print X_all.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection\n",
    "Since we have 22 features for every record in the dataset, let's analyse which features are irrelevant and can be removed from it. We will run a simple decision tree classifier on every feature and report how good the prediction is. Those features that can already be predicted, are not necessary for our target prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cap-shape : 0.101920236337\n",
      "cap-surface : 0.232890201871\n",
      "cap-color : 0.110782865583\n",
      "bruises : 1.0\n",
      "odor : 0.70113244707\n",
      "gill-attachment : 0.997045790251\n",
      "gill-spacing : 0.981290004924\n",
      "gill-size : 1.0\n",
      "gill-color : 0.238306253077\n",
      "stalk-shape : 1.0\n",
      "stalk-root : 1.0\n",
      "stalk-surface-above-ring : 0.663220088626\n",
      "stalk-surface-below-ring : 0.65780403742\n",
      "stalk-color-above-ring : 0.439684884293\n",
      "stalk-color-below-ring : 0.439684884293\n",
      "veil-type : 1.0\n",
      "veil-color : 0.977843426883\n",
      "ring-number : 1.0\n",
      "ring-type : 1.0\n",
      "spore-print-color : 0.563269325455\n",
      "population : 0.374692269818\n",
      "habitat : 0.472673559823\n"
     ]
    }
   ],
   "source": [
    "def test_feature_accuracy(X):\n",
    "    \n",
    "    for col, col_data in X.iteritems():\n",
    "        target_feature = X[col]\n",
    "        new_x = X.drop(col, axis=1)\n",
    "    \n",
    "        from sklearn.cross_validation import train_test_split\n",
    "        X_train, X_test, y_train, y_test = train_test_split(new_x, target_feature, test_size=0.25, random_state=42)\n",
    "\n",
    "        from sklearn.tree import DecisionTreeClassifier\n",
    "        classifier = DecisionTreeClassifier(random_state=42)\n",
    "        classifier.fit(X_train, y_train)\n",
    "\n",
    "        score = classifier.score(X_test, y_test)\n",
    "        \n",
    "        if score>0.9:\n",
    "            X = X.drop(col, axis=1)\n",
    "\n",
    "        print \"{} : {}\".format(col, score)\n",
    "        \n",
    "    return  X\n",
    "    \n",
    "    \n",
    "X_all = test_feature_accuracy(X_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After removing those cells that can be predicted with score greater than 0.9, we now have 12 features and the number of features is significally reduced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing PCA for additional dimensionality reduction\n",
    "Now, we can use PCA for additional dimensionality reduction. Let's see how many components will give us decent explained variance ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.926939376005\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=8)\n",
    "pca.fit(X_all)\n",
    "\n",
    "print np.sum(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After a bit testing, it is possible to see that we can reduce the number of features to 8 principal components which will result in 0.9269 explained variance ratio. We can use this to turn this problem into binary clustering, and then try to minimise the error. Since we will turn our feature values into continuous values, it will be hard to achieve 100% with this type of problem we are tackling. We will first try to perform traditional supervised learning, and then if there is need to speed up the training/testing time as well as reduce variance and bias, we can come back to PCA dimensionality reduction and reduce the number of features for that sake."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Evaluating Models\n",
    "Now, our data is prepared and we can begin modeling, training and evaluating our models. We will choose 3 supervised learning models, and try fitting our data to them. Then, we will see how do they perform and tune them accordingly. For those classifiers that have class_weight and sample_weight parameters, if there is need, we will tune them in order to implement cost sensitive learning (for example, it is better to predict edible mushroom as poisonous than poisonous mushroom as edible). We will evaluate our model with accuracy score and produce the tables which show the training set size, time, prediction time, accuracy score on training and accuracy score on testing set. For this problem, we will choose the following three algorithms:\n",
    "\n",
    "- SVM\n",
    "- Stochastic Gradient Descent\n",
    "- Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Support Vector Machines\n",
    "\n",
    "The reason I choose SVMs is that the real world application of them is binary classification. Since we are dealing with binary classification in this problem, we will expect this model to perform well. The idea behind SVM is to find the greatest margin between two different sets. It is computing it mathematically to find the hyperplane which will separate the data with the greatest margin. If the data is not linearly separable in 2D, it will hit up higher dimension and then try to separate it. SVM performs poorly with large datasets and with datasets with lots of noise. This is not the case with our problem, and thus we can expect good results.\n",
    "\n",
    "- Stochastic Gradient Descent\n",
    "\n",
    "The Stochastic Gradient Descent is based on neural networks and the best way to use it is in handwriting recognition and such problems. This model should not be a good fit for our type of problem, but we will test it to compare its results with the other two. The algorithm is based on estimating the gradient by computing the part of the gradient for a small sample of randomly chosen training inputs.By averaging over this small sample it turns out that we can quickly get a good estimate of the true gradient, and this helps speed up gradient descent, and thus learning. Pros are that it is a fairly well studied algorithm, so for the most part the problems with GD have solutions. However, sometimes calculating the gradient can be very expensive or intractable if the size of our data is large.\n",
    "\n",
    "- Random Forest\n",
    "\n",
    "While reading papers regarding this problem we are tackling, we have seen that the decision trees can give very high accuracy scores while predicting the mushrooms as edible or poisonous. Random forest is an ensemble of decision trees. Each decision tree is constructed by using a random subset of the training data. After we have trained our forest, we can pass each test row through it, in order to output a prediction. This model should perform the best because it is based on decision trees, and decision trees is very good fit for this kind of problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "Let's create three helper functions which we will be using for training and testing our models. \n",
    "Create confusion matrix function is pretty straightforward. It creates the confusion matrix based on true and predicted values. \n",
    "Train classifier function will take a classifier as a param and train it with the provided data. \n",
    "Predict labels will take as input fit classifier, features and a target labeling. Then it will make predictions using the accuracy score. \n",
    "Train predict will take as input a classifier, training and testing data and it will perform train classifier and predict labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_confusion_matrix(true, pred):\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    print \"----------------\"\n",
    "    print \"Confusion matrix\\n\"\n",
    "    print confusion_matrix(true, pred)\n",
    "    print \"\\n----------------\\n\"\n",
    "\n",
    "def train_classifier(clf, X_train, y_train):\n",
    "\n",
    "    start = time()\n",
    "    clf.fit(X_train, y_train)\n",
    "    end = time()\n",
    "    print \"Trained model in {:.4f} seconds\".format(end - start)\n",
    "    \n",
    "def predict_labels(clf, features, target, confusion_matrix=False):\n",
    "\n",
    "    start = time()\n",
    "    y_pred = clf.predict(features)\n",
    "    end = time()\n",
    "\n",
    "    print \"Made predictions in {:.4f} seconds.\".format(end - start)\n",
    "    \n",
    "    if confusion_matrix:\n",
    "        create_confusion_matrix(target.values, y_pred)\n",
    "    \n",
    "    return accuracy_score(target.values, y_pred)\n",
    "\n",
    "\n",
    "def train_predict(clf, X_train, y_train, X_test, y_test, confusion_matrix=False):\n",
    "\n",
    "    print \"Training a {} using a training set size of {}. . .\".format(clf.__class__.__name__, len(X_train))\n",
    "    \n",
    "    train_classifier(clf, X_train, y_train)\n",
    "    \n",
    "    print \"Accuracy score for training set: {:.4f}.\".format(predict_labels(clf, X_train, y_train, confusion_matrix))\n",
    "    print \"Accuracy score for test set: {:.4f}.\".format(predict_labels(clf, X_test, y_test, confusion_matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation: Model Performance Metrics\n",
    "Now we will import our supervised learning models and run the train_predict function for each one. We will use different training set sizes (100, 200, 300). Details such as prediction time, accuracy scores, training time etc. will be contained in the tabular results below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SVC: \n",
      "\n",
      "Training a SVC using a training set size of 100. . .\n",
      "Trained model in 0.0026 seconds\n",
      "Made predictions in 0.0005 seconds.\n",
      "Accuracy score for training set: 1.0000.\n",
      "Made predictions in 0.0079 seconds.\n",
      "Accuracy score for test set: 0.8922.\n",
      "Training a SVC using a training set size of 200. . .\n",
      "Trained model in 0.0048 seconds\n",
      "Made predictions in 0.0013 seconds.\n",
      "Accuracy score for training set: 0.9900.\n",
      "Made predictions in 0.0098 seconds.\n",
      "Accuracy score for test set: 0.9222.\n",
      "Training a SVC using a training set size of 300. . .\n",
      "Trained model in 0.0055 seconds\n",
      "Made predictions in 0.0027 seconds.\n",
      "Accuracy score for training set: 0.9933.\n",
      "Made predictions in 0.0157 seconds.\n",
      "Accuracy score for test set: 0.9473.\n",
      "\n",
      "SGDClassifier: \n",
      "\n",
      "Training a SGDClassifier using a training set size of 100. . .\n",
      "Trained model in 0.0009 seconds\n",
      "Made predictions in 0.0002 seconds.\n",
      "Accuracy score for training set: 0.5900.\n",
      "Made predictions in 0.0003 seconds.\n",
      "Accuracy score for test set: 0.5574.\n",
      "Training a SGDClassifier using a training set size of 200. . .\n",
      "Trained model in 0.0006 seconds\n",
      "Made predictions in 0.0001 seconds.\n",
      "Accuracy score for training set: 0.6650.\n",
      "Made predictions in 0.0002 seconds.\n",
      "Accuracy score for test set: 0.6539.\n",
      "Training a SGDClassifier using a training set size of 300. . .\n",
      "Trained model in 0.0006 seconds\n",
      "Made predictions in 0.0001 seconds.\n",
      "Accuracy score for training set: 0.8500.\n",
      "Made predictions in 0.0001 seconds.\n",
      "Accuracy score for test set: 0.8518.\n",
      "\n",
      "RandomForestClassifier: \n",
      "\n",
      "Training a RandomForestClassifier using a training set size of 100. . .\n",
      "Trained model in 0.0237 seconds\n",
      "Made predictions in 0.0009 seconds.\n",
      "Accuracy score for training set: 1.0000.\n",
      "Made predictions in 0.0020 seconds.\n",
      "Accuracy score for test set: 0.9517.\n",
      "Training a RandomForestClassifier using a training set size of 200. . .\n",
      "Trained model in 0.0231 seconds\n",
      "Made predictions in 0.0010 seconds.\n",
      "Accuracy score for training set: 0.9950.\n",
      "Made predictions in 0.0025 seconds.\n",
      "Accuracy score for test set: 0.9572.\n",
      "Training a RandomForestClassifier using a training set size of 300. . .\n",
      "Trained model in 0.0231 seconds\n",
      "Made predictions in 0.0012 seconds.\n",
      "Accuracy score for training set: 1.0000.\n",
      "Made predictions in 0.0020 seconds.\n",
      "Accuracy score for test set: 0.9838.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf_A = SVC(random_state=42)\n",
    "clf_B = SGDClassifier(random_state=42)\n",
    "clf_C = RandomForestClassifier()\n",
    "\n",
    "for clf in [clf_A, clf_B, clf_C]:\n",
    "    print \"\\n{}: \\n\".format(clf.__class__.__name__)\n",
    "    for n in [100, 200, 300]:\n",
    "        train_predict(clf, X_train[:n], y_train[:n], X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tabular Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Classifer 1 - SVC **  \n",
    "\n",
    "| Training Set Size | Training Time | Prediction Time (test) | Accuracy Score (train) | Accuracy Score (test) |\n",
    "| :---------------: | :---------------------: | :--------------------: | :--------------: | :-------------: |\n",
    "| 100               | 0.0012                  | 0.0005                 |   1.0000         | 0.8922          |\n",
    "| 200               | 0.0032                  | 0.0011                 |   0.9900         | 0.9222          |\n",
    "| 300               | 0.0044                  | 0.0026                 |   0.9933         | 0.9473          |\n",
    "\n",
    "** Classifer 2 - Stochastic Gradient Descent Classifier **  \n",
    "\n",
    "| Training Set Size | Training Time | Prediction Time (test) | Accuracy Score (train) | Accuracy Score (test) |\n",
    "| :---------------: | :---------------------: | :--------------------: | :--------------: | :-------------: |\n",
    "| 100               | 0.0006                  | 0.0001                 | 0.5900           | 0.5574          |\n",
    "| 200               | 0.0006                  | 0.0001                 | 0.6650           | 0.6539          |\n",
    "| 300               | 0.0006                  | 0.0002                 | 0.8500           | 0.8518          |\n",
    "\n",
    "** Classifer 3 - Random Forests Classifier **  \n",
    "\n",
    "| Training Set Size | Training Time | Prediction Time (test) | Accuracy Score (train) | Accuracy Score (test) |\n",
    "| :---------------: | :---------------------: | :--------------------: | :--------------: | :-------------: |\n",
    "| 100               | 0.0237                  | 0.0009                 | 1.0000           | 0.9581          |\n",
    "| 200               | 0.0228                  | 0.0009                 | 1.0000           | 0.9695          |\n",
    "| 300               | 0.0227                  | 0.0010                 | 1.0000           | 0.9665          |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing the Best Model\n",
    "Based on the experiments performed earlier, we have confirmed our assumptions about each model. Random Forests performs the best, as it is ensemble of decision trees. SVMs are performing well as we are dealing with the binary classification here, and Stochastic Gradient Descent is the fastest, but the accuracy score is very low compared to the other two models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Tuning\n",
    "Now, it is time to apply first iteration of tuning our Random Forests model. We will be using grid search (`GridSearchCV`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Made predictions in 0.0044 seconds.\n",
      "----------------\n",
      "Confusion matrix\n",
      "\n",
      "[[3168    0]\n",
      " [   0 2925]]\n",
      "\n",
      "----------------\n",
      "\n",
      "Tuned model has a training accuracy score of 1.0000.\n",
      "Made predictions in 0.0020 seconds.\n",
      "----------------\n",
      "Confusion matrix\n",
      "\n",
      "[[1040    0]\n",
      " [   0  991]]\n",
      "\n",
      "----------------\n",
      "\n",
      "Tuned model has a testing accuracy score of 1.0000.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "parameters = {'max_depth': [2,4,6,10,15], 'min_samples_leaf': [2,5,10], 'max_features':[1,2,10]}\n",
    " \n",
    "clf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "accuracy_scorer = make_scorer(accuracy_score)\n",
    "\n",
    "grid_obj = GridSearchCV(clf, parameters, scoring=accuracy_scorer)\n",
    "\n",
    "grid_obj = grid_obj.fit(X_train, y_train)\n",
    "\n",
    "clf = grid_obj.best_estimator_\n",
    "\n",
    "print \"Tuned model has a training accuracy score of {:.4f}.\".format(predict_labels(clf, X_train, y_train, True))\n",
    "print \"Tuned model has a testing accuracy score of {:.4f}.\".format(predict_labels(clf, X_test, y_test, True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrices\n",
    "\n",
    "- Confusion Matrix for training\n",
    "\n",
    "| True Positive | True Negative | False Positive | False Negative |\n",
    "| :---------------:| :--------------------:| :---------------: | :---------------------: |\n",
    "| 3168               | 2925                  | 0               | 0                  |\n",
    "\n",
    "\n",
    "- Confusion Matrix for testing\n",
    "\n",
    "| True Positive | True Negative | False Positive | False Negative |\n",
    "| :---------------:| :--------------------:| :---------------: | :---------------------: |\n",
    "| 1040               | 991                  | 0               | 0                  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After tuning our Random Forest model, we have achieved exceptional results. We have successfully obtained 100% accuracy score on both training and testing sets with satisfiable prediction time of 0.0044 and 0.0020 seconds. Since we managed to to successfully classify 100% of mushrooms both in training and testing set, the additional dimensionality reduction with PCA is not necessary. Random Forests model is very quick and accurate."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
